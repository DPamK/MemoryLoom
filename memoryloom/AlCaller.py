import time
import uuid
import httpx
import litellm

class AICaller:
    def __init__(self, 
                 model_name: str,
                 api_token: str = 'token-abc123',
                 api_base: str = None,
                 proxy: str = None,
                 max_tokens: int = 4096):
        """
        Initializes an instance of the AICaller class.

        Parameters:
            model_name (str): The name of the language model to be used.
            api_token (str, optional): The API token for accessing the language model. Defaults to 'token-abc123'.
            api_base (str, optional): The base URL for accessing the language model API. Defaults to None.
            proxy (str, optional): The URL of the proxy to be used for API requests. Defaults to None.
        """
        self.model = model_name
        print(f"Initializing AICaller {self.model}...")
        self.api_token = api_token
        self.api_base = api_base if api_base else ""
        self.max_tokens = max_tokens
        if proxy:
            self.use_proxy = True
            self.proxy_url = proxy
        else:
            self.use_proxy = False
            self.proxy_url = None

    def chat(self, 
             query: str, 
             system_prompt: str = "You are a helpful assistant.",
             stream: bool = False,
             max_tokens: int = 0,
             tempture: float = 0.7,
             top_p: float = 0.7):
        """
        Calls the language model with the given query and system prompt.
        Parameters:
            query (str): The query to be sent to the language model.
            system_prompt (str, optional): The system prompt to be used. Defaults to "You are a helpful assistant.".
        Returns:
            str: The response generated by the language model.
        """
        if type(query) == list:
            messages = query
        else:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": query}
            ]

        if self.use_proxy:
            litellm.client_session = httpx.Client(proxy=self.proxy_url)
        
        response = litellm.completion(
            model=self.model,
            messages=messages,
            api_key=self.api_token,
            api_base=self.api_base,
            max_tokens=self.max_tokens if max_tokens == 0 else max_tokens,
            stream=stream,
            temperature=tempture,
            top_p=top_p
        )
        if stream:
            chunks = []
            try:
                for chunk in response:
                    chunks.append(chunk)
                    time.sleep(0.01)
            except Exception as e:
                print(f"Error during streaming: {e}")

            model_response = litellm.stream_chunk_builder(
                chunks, messages=messages)
        else:
            model_response = response

        return model_response["choices"][0]["message"]["content"]

