import time
import uuid
import httpx
import litellm

class AICaller:
    def __init__(self, 
                 model_name: str,
                 api_token: str = 'token-abc123',
                 api_base: str = None,
                 proxy: str = None,
                 max_tokens: int = 4096):
        """
        Initializes an instance of the AICaller class.

        Parameters:
            model_name (str): The name of the language model to be used.
            api_token (str, optional): The API token for accessing the language model. Defaults to 'token-abc123'.
            api_base (str, optional): The base URL for accessing the language model API. Defaults to None.
            proxy (str, optional): The URL of the proxy to be used for API requests. Defaults to None.
        """
        self.model = model_name
        print(f"Initializing AICaller {self.model}...")
        self.api_token = api_token
        self.api_base = api_base if api_base else ""
        self.max_tokens = max_tokens
        if proxy:
            self.use_proxy = True
            self.proxy_url = proxy
        else:
            self.use_proxy = False
            self.proxy_url = None

    def chat(self, 
             query: str, 
             system_prompt: str = "You are a helpful assistant.",
             stream: bool = False,
             max_tokens: int = 0,
             tempture: float = 0.7,
             top_p: float = 0.7):
        """
        Calls the language model with the given query and system prompt.
        Parameters:
            query (str): The query to be sent to the language model.
            system_prompt (str, optional): The system prompt to be used. Defaults to "You are a helpful assistant.".
        Returns:
            str: The response generated by the language model.
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": query}
        ]
        if self.use_proxy:
            litellm.client_session = httpx.Client(proxy=self.proxy_url)
        
        response = litellm.completion(
            model=self.model,
            messages=messages,
            api_key=self.api_token,
            api_base=self.api_base,
            max_tokens=self.max_tokens if max_tokens == 0 else max_tokens,
            stream=stream,
            temperature=tempture,
            top_p=top_p
        )
        if stream:
            chunks = []
            try:
                for chunk in response:
                    chunks.append(chunk)
                    time.sleep(0.01)
            except Exception as e:
                print(f"Error during streaming: {e}")

            model_response = litellm.stream_chunk_builder(
                chunks, messages=messages)
        else:
            model_response = response

        return model_response["choices"][0]["message"]["content"]


    def call_model(self, 
                prompt: dict, 
                stream: bool = False,
                max_tokens: int = 0,
                tempture: float = 0.7,
                top_p: float = 0.7):
        """
        Call the language model with the provided prompt and retrieve the response.

        Parameters:
            prompt (dict): The prompt to be sent to the language model.
            max_tokens (int, optional): The maximum number of tokens to generate in the response. Defaults to 4096.

        Returns:
            tuple: A tuple containing the response generated by the language model, the number of tokens used from the prompt, and the total number of tokens in the response.
        """
        if "system" not in prompt or "user" not in prompt:
            raise KeyError(
                "The prompt dictionary must contain 'system' and 'user' keys."
            )
        # Default Completion parameters
        completion_params = {
            "model": self.model_name,
            "messages": prompt,
            "max_tokens": self.max_tokens if max_tokens == 0 else max_tokens,
            "stream": stream,
            "temperature": tempture,
            "top_p": top_p,
            "api_key": self.api_token,
        }
        if self.use_proxy:
            litellm.client_session = httpx.Client(proxy=self.proxy_url)

        response = litellm.completion(**completion_params)

        if stream:
            chunks = []
            try:
                for chunk in response:
                    chunks.append(chunk)
                    time.sleep(0.01)
            except Exception as e:
                print(f"Error during streaming: {e}")

            model_response = litellm.stream_chunk_builder(
                chunks, messages=prompt)
        else:
            model_response = response

        return (
            model_response["choices"][0]["message"]["content"],
            int(model_response["usage"]["prompt_tokens"]),
            int(model_response["usage"]["completion_tokens"]),
        )
